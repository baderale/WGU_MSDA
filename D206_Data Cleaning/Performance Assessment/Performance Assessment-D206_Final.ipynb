{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Performance Assessment:Data Cleaning (NUM3)</center></h1>\n",
    "<h3><center> by Bader Ale <center><h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Performance Assessment, I will be using the medical data contained in the D206 Definitions and Datafile directory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Research Question and Variables\n",
    "The research question for this analysis is:\n",
    "**Is there a relation between the amount of times the primary physician visited the patient during their hospital stay and the occurence of readmission within 30 days following the patient's discharge from the facility?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we have to do is import the original CSV file that contains our data. To do this, we must first import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the original CSV file\n",
    "df = pd.read_csv(r'C:\\Users\\Bader Ale\\Documents\\WGU_MSDA\\D206_Data Cleaning\\Performance Assessment\\medical_raw_data.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing our CSV file, we will see the first 5 records of our dataframe and see the overall shape/size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning first 5 records of dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning number of (rows, columns)\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframe has a total of 10,000 rows and 53 columns. Next, we will return a list of all variables and their dataypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return variables, datatypes and non-null status of each.\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Detection and Treatment of Duplicates\n",
    "Our first task is to detect and treat any duplicated values in our entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning a total count of duplicated values\n",
    "df.duplicated().value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see there are **no** duplicated values, represented by the \"False 10000\" output (which is also the total rows shown in the .shape fucntion). We can now move to the next section of data cleaning, detection and treatment of missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Detection and Treatment of Missing Values\n",
    "In this section, we will see if there are any missing values for all variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning a list of variables with total counts for missing values \n",
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there are 7 columns with missing values; **children**, **age**, **income**, **soft_drink**, **overweight**, **anxiety** and **initial_days**\n",
    "<br>\n",
    "<br>1) **Children** and **Age** are considered *discrete quantitative variables* because they can only be particular numbers\n",
    "<br>2) **Income** and **Initial_days** are considered *continuous quantitative variables* because your income can be whole numbers or contain decimals\n",
    "<br>3) **Overweight**, **Soft_drink** and **Anxiety** are considered *nominal qualitative variables* because they are either yes or no\n",
    "\n",
    "We will return some basic statistics on the quantitative variables to check the before and after imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking statistical information on the columns with missing data that are quantative\n",
    "df[['Children', 'Age', 'Income','Initial_days']].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the seaborn package, we can create histograms of the quantitative variables **Children**, **Age**, **Income**, **Intial_days** to visually\n",
    "analyze their distribution, but before we must import the seaborn package into our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing seaborn package with the inline magic function\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting histograms\n",
    "sns.displot(df, x='Children')\n",
    "sns.displot(df, x='Age')\n",
    "sns.displot(df, x='Income')\n",
    "sns.displot(df, x='Initial_days')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these graphs, we can see that:\n",
    "\n",
    "1) Both **Children** and **Income** are positively skewed to the right\n",
    "2) **Age** is uniformly distributed \n",
    "3) **Initial_days** has a bimodal distribution\n",
    "\n",
    "For **Income**, **Children** and **Initial_days** variables, we will treat missing values by imputation using the median value while for **Age** we will be using the mean for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing imputation \n",
    "df['Children'].fillna(df['Children'].median(), inplace= True) # Using median value for Children\n",
    "df['Income'].fillna(df['Income'].median(), inplace= True) # Using Median value for Income\n",
    "df['Initial_days'].fillna(df['Initial_days'].median(), inplace= True) # Using median value for Initial_days\n",
    "df['Age'].fillna(df['Age'].mean(), inplace= True) # Performing imputation using mean values form Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking statistics again for comparison\n",
    "df[['Children', 'Age', 'Income','Initial_days']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms again to check for skewness\n",
    "#Plotting histograms\n",
    "sns.displot(df, x='Children')\n",
    "sns.displot(df, x='Age')\n",
    "sns.displot(df, x='Income')\n",
    "sns.displot(df, x='Initial_days')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the data behavior is relatively conserved, evident by both statistical information and histogram.\n",
    "Now we will focus on the remaining categorical variables **Overweight**, **Anxiety**, and **Soft_drink**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting amount of missing values in each\n",
    "print(df['Overweight'].isnull().value_counts())\n",
    "print('') \n",
    "print(df['Soft_drink'].isnull().value_counts())\n",
    "print('')\n",
    "print(df['Anxiety'].isnull().value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with the **Overweight** column - the **Overweight** columns is of the \"0 or 1\" kind. In order to treat missing values, we will calculate the percentage of each category ( 0 and 1) and impute missing values with the highest percentage category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining value counts for each category in the Overweight column\n",
    "df['Overweight'].value_counts(normalize=True, sort=True) # We are using the parameter normalize to return the proportions rather than frequencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a nearly 70/30 split for 1:0 ratio - this means we can impute the missing values using '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating missing values in Overweight column with '1' (highest percentage category)\n",
    "df['Overweight'].fillna(1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will follow the same procedure for the **Anxiety** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining value counts for each category in the Overweight column\n",
    "df['Anxiety'].value_counts(normalize=True, sort=True) # We are using the parameter normalize to return the proportions rather than frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating missing values in Anxiety column with '0' (highest percentage category)\n",
    "df['Anxiety'].fillna(0, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now focus on the **Soft-Drink** column which is of the 'Yes/No' type - we will first re-express the variable to '0/1' using Ordinal Encoding and then using the methods used above for **Overweight** and **Anxiety** to fill is missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will run the .unique() function to determine number of unique values for specified variables\n",
    "df['Soft_drink'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the unique values are 'No', 'Yes' and _nan_ for missing values. We will use the percentage method mentioned above to fill these in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining value counts for each category in the Overweight column\n",
    "df['Soft_drink'].value_counts(normalize=True, sort=True, dropna=True) # not including the NaN values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a 75/25 split for a 1:0 ratio - we will use 'No' to fill in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating missing values in Soft_drink column with 'No' (highest percentage category)\n",
    "df['Soft_drink'].fillna('No', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechecking for any missing values\n",
    "df['Soft_drink'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now begin the ordinal encoding process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate the variable in preparation for replacing its categorical values with\n",
    "# numeric ones. This replicated variable will store the re-expressed values once converted\n",
    "\n",
    "df['Soft_drink_numeric'] = df['Soft_drink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking duplicated column 'Soft_drink_numeric' vs original \"Soft_drink\"\n",
    "df[['Soft_drink_numeric', 'Soft_drink']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary specifically for converting the categorical values to numeric values.\n",
    "dict_soft_drink = {'Soft_drink_numeric': {'No': 0, 'Yes': 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the dictionary to replace the variableâ€™s values. The replacefunction will replace the values according to the rules in\n",
    "# the dictionary dict_edu and store in existing data frame.\n",
    "df.replace(dict_soft_drink, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have detected and treated all missing values. We will now see the dataframe overall and run .info() to see if any null-values exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking info on dataframe\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, there are no more variables with missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Detection and Treatment of Outliers\n",
    "In this section, we will check for any outliers in the quantitative variables and treat them accordingly.\n",
    "Let's first visualize the boxplots for the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Children boxplot\n",
    "sns.boxplot(x='Children', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age boxplot\n",
    "sns.boxplot(x='Age', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income boxplot\n",
    "sns.boxplot(x='Income', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial_days boxplot\n",
    "sns.boxplot(x='Initial_days', data=df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that **Age** and **Initial_days** have no outliers while **Children** has 4 outliers and **Income** has multiple. We will treat the outliers as follows:\n",
    "\n",
    "1) We will delete the 4 outliers in **Children** \n",
    "2) We will exclude the outliers in the **Income** column and save it as a seperate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Income'].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we see our max value in the **Income** column, we will use the z-scores to extract all records whose z-score is greater than 3. We must first import the SciPy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improting Scipy package\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column for the Income z-scores\n",
    "df['Income_z_Scores'] = stats.zscore(df['Income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing first 10 records for both Income and Income_z_Scores columns\n",
    "df[['Income', 'Income_z_Scores']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting records with z-scores -3 < z and z > 3 and saving as new variable\n",
    "income_outliers = df.query('Income_z_Scores < -3 | Income_z_Scores > 3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23ecf8896591abdb4f319a996273d77760c5f800ba007ba21e578a28215e94bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
