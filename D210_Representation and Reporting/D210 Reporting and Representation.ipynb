{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D210 - Reporting and Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('medical_clean.csv', index_col='Customer_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = df[['Item1', 'Item2', 'Item3', 'Item4', 'Item5', 'Item6', 'Item7', 'Item8']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df.rename(columns={'Item1':'Timely_Admission', 'Item2':'Timely_Treatment', 'Item3':'Timely_Visits', 'Item4':'Reliability',\n",
    "                          'Item5':'Options', 'Item6':'HoursofTreatment', 'Item7':'Courteous_Staff', 'Item8':'Active_Listening_Doctor'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Item1','Item2','Item3','Item4','Item5','Item6','Item7','Item8'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# § Data Cleaning  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ▶ Detection and Treatment of Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ▶ Detection and Treatment of Duplicated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# § Readmission Prediction using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y data\n",
    "X = df[['Area','Income','Marital','Gender','VitD_levels','Doc_visits',\n",
    "       'Full_meals_eaten','vitD_supp','Soft_drink','Initial_admin',\n",
    "       'HighBlood','Stroke','Complication_risk','Overweight','Arthritis',\n",
    "       'Diabetes','Hyperlipidemia','BackPain','Anxiety','Allergic_rhinitis',\n",
    "       'Reflux_esophagitis','Asthma','Services','Initial_days','TotalCharge',\n",
    "        'Additional_charges']]\n",
    "y = df['ReAdmis'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.get_dummies(data=X, columns=['Area','Marital','Gender','Soft_drink',\n",
    "                                    'Initial_admin','HighBlood','Stroke','Overweight','Arthritis',\n",
    "                                    'Diabetes','Hyperlipidemia','BackPain','Anxiety','Allergic_rhinitis',\n",
    "                                    'Reflux_esophagitis','Asthma','Services'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding colum \"Complication Risk\"\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "X['Complication_risk'] = enc.fit_transform(X[['Complication_risk']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=53)\n",
    "\n",
    "pipe = Pipeline([('model', RandomForestClassifier(random_state=53))])\n",
    "\n",
    "pipe.get_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = GridSearchCV(estimator=pipe,\n",
    "                   param_grid={'model__max_depth': [1,2,3,4,5,6,7,8,9,10]},\n",
    "                   cv=5,\n",
    "                   n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The best depth for the Random Forest is: {mod.best_params_}')\n",
    "print(f'The best score was : {mod.best_score_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize the confusion matrix using a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the confusion matrix for the model. It shows:  \n",
    "1) **1898** - These are the cases where the model correctly predicted the positive class (e.g., a patient is readmitted to the hospital), and the actual outcome was also **positive**.\n",
    "2) **1042** - These are the cases where the model correctly predicted the negative class (e.g., a patient is not readmitted to the hospital), and the actual outcome was also **negative**.\n",
    "3) **28** - These are the cases where the model incorrectly predicted the positive class when it should have been negative. In other words, the model gave a positive prediction, but the actual outcome was negative.\n",
    "4) **32** - These are the cases where the model incorrectly predicted the negative class when it should have been positive. The model gave a negative prediction, but the actual outcome was positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report for additional performance metrics\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification reports breaks down the model's performence further. Some key takeaways are:\n",
    "\n",
    "1. **Precision**:\n",
    "   - For the \"No\" class: Precision is 0.99, which means that when the model predicts \"No\" (negative class), it is correct 99% of the time.\n",
    "   - For the \"Yes\" class: Precision is 0.97, indicating that when the model predicts \"Yes\" (positive class), it is correct 97% of the time.  \n",
    "2. **Recall (Sensitivity)**:\n",
    "   - For the \"No\" class: Recall is 0.98, meaning that the model correctly identifies 98% of the actual \"No\" cases.\n",
    "   - For the \"Yes\" class: Recall is 0.97, indicating that the model captures 97% of the actual \"Yes\" cases.\n",
    "3. **F1-Score**:\n",
    "   - For the \"No\" class, the F1-score is 0.98, which is a harmonic mean of precision and recall. It provides a balanced measure of accuracy.\n",
    "   - For the \"Yes\" class, the F1-score is 0.97, reflecting the balance between precision and recall for the \"Yes\" class.\n",
    "4. **Support**:\n",
    "   - The \"support\" column shows the number of instances in each class in the test dataset.\n",
    "     - For the \"No\" class, there are 1,930 instances.\n",
    "     - For the \"Yes\" class, there are 1,070 instances.\n",
    "5. **Accuracy**:\n",
    "   - The overall accuracy of the model is 0.98, or 98%. This indicates that 98% of the predictions (both \"Yes\" and \"No\" combined) are correct.\n",
    "6. **Macro Avg**:\n",
    "   - The \"macro avg\" row shows the average of precision, recall, and F1-score for both classes. In this case, the average is 0.98.\n",
    "7. **Weighted Avg**:\n",
    "   - The \"weighted avg\" row provides a weighted average of precision, recall, and F1-score. It takes into account the class imbalances, giving more weight to the class with more samples. In this case, the weighted average is 0.98.\n",
    "\n",
    "In summary, the model appears to perform very well, with high precision, recall, and F1-scores for both the \"Yes\" and \"No\" classes. The high accuracy of 98% suggests that the model is effective at correctly classifying instances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
