{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Performance Assessment:Exploratory Data Analysis (OEM2)</center></h1>\n",
    "<h3><center> by Bader Ale <center><h3>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Performance Assessment, I will be using the medical data contained in the D207 Definitions and Datafile directory."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 1: Research Question and Variables\n",
    "The research question for this analysis is:\n",
    "**Is there a relation between the amount of times the primary physician visited the patient during their hospital stay and the occurence of readmission within 30 days following the patient's discharge from the facility?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we have to do is import the original CSV file that contains our data. To do this, we must first import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from IPython.core.interactiveshell import InteractiveShell # Importing so we can run multiple lines in one cell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # Code so multiple lines in one cell can be ran simultaenously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the original CSV file\n",
    "df = pd.read_csv(r'F:\\GitHub Repos\\WGU_MSDA\\D207_Exploratory Data Analysis\\medical_clean.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing our CSV file, we will see the first 5 records of our dataframe and see the overall shape/size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CaseOrder</th>\n",
       "      <th>Customer_id</th>\n",
       "      <th>Interaction</th>\n",
       "      <th>UID</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>Zip</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Lng</th>\n",
       "      <th>...</th>\n",
       "      <th>TotalCharge</th>\n",
       "      <th>Additional_charges</th>\n",
       "      <th>Item1</th>\n",
       "      <th>Item2</th>\n",
       "      <th>Item3</th>\n",
       "      <th>Item4</th>\n",
       "      <th>Item5</th>\n",
       "      <th>Item6</th>\n",
       "      <th>Item7</th>\n",
       "      <th>Item8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>C412403</td>\n",
       "      <td>8cd49b13-f45a-4b47-a2bd-173ffa932c2f</td>\n",
       "      <td>3a83ddb66e2ae73798bdf1d705dc0932</td>\n",
       "      <td>Eva</td>\n",
       "      <td>AL</td>\n",
       "      <td>Morgan</td>\n",
       "      <td>35621</td>\n",
       "      <td>34.34960</td>\n",
       "      <td>-86.72508</td>\n",
       "      <td>...</td>\n",
       "      <td>3726.702860</td>\n",
       "      <td>17939.403420</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Z919181</td>\n",
       "      <td>d2450b70-0337-4406-bdbb-bc1037f1734c</td>\n",
       "      <td>176354c5eef714957d486009feabf195</td>\n",
       "      <td>Marianna</td>\n",
       "      <td>FL</td>\n",
       "      <td>Jackson</td>\n",
       "      <td>32446</td>\n",
       "      <td>30.84513</td>\n",
       "      <td>-85.22907</td>\n",
       "      <td>...</td>\n",
       "      <td>4193.190458</td>\n",
       "      <td>17612.998120</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>F995323</td>\n",
       "      <td>a2057123-abf5-4a2c-abad-8ffe33512562</td>\n",
       "      <td>e19a0fa00aeda885b8a436757e889bc9</td>\n",
       "      <td>Sioux Falls</td>\n",
       "      <td>SD</td>\n",
       "      <td>Minnehaha</td>\n",
       "      <td>57110</td>\n",
       "      <td>43.54321</td>\n",
       "      <td>-96.63772</td>\n",
       "      <td>...</td>\n",
       "      <td>2434.234222</td>\n",
       "      <td>17505.192460</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A879973</td>\n",
       "      <td>1dec528d-eb34-4079-adce-0d7a40e82205</td>\n",
       "      <td>cd17d7b6d152cb6f23957346d11c3f07</td>\n",
       "      <td>New Richland</td>\n",
       "      <td>MN</td>\n",
       "      <td>Waseca</td>\n",
       "      <td>56072</td>\n",
       "      <td>43.89744</td>\n",
       "      <td>-93.51479</td>\n",
       "      <td>...</td>\n",
       "      <td>2127.830423</td>\n",
       "      <td>12993.437350</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>C544523</td>\n",
       "      <td>5885f56b-d6da-43a3-8760-83583af94266</td>\n",
       "      <td>d2f0425877b10ed6bb381f3e2579424a</td>\n",
       "      <td>West Point</td>\n",
       "      <td>VA</td>\n",
       "      <td>King William</td>\n",
       "      <td>23181</td>\n",
       "      <td>37.59894</td>\n",
       "      <td>-76.88958</td>\n",
       "      <td>...</td>\n",
       "      <td>2113.073274</td>\n",
       "      <td>3716.525786</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CaseOrder Customer_id                           Interaction  \\\n",
       "0          1     C412403  8cd49b13-f45a-4b47-a2bd-173ffa932c2f   \n",
       "1          2     Z919181  d2450b70-0337-4406-bdbb-bc1037f1734c   \n",
       "2          3     F995323  a2057123-abf5-4a2c-abad-8ffe33512562   \n",
       "3          4     A879973  1dec528d-eb34-4079-adce-0d7a40e82205   \n",
       "4          5     C544523  5885f56b-d6da-43a3-8760-83583af94266   \n",
       "\n",
       "                                UID          City State        County    Zip  \\\n",
       "0  3a83ddb66e2ae73798bdf1d705dc0932           Eva    AL        Morgan  35621   \n",
       "1  176354c5eef714957d486009feabf195      Marianna    FL       Jackson  32446   \n",
       "2  e19a0fa00aeda885b8a436757e889bc9   Sioux Falls    SD     Minnehaha  57110   \n",
       "3  cd17d7b6d152cb6f23957346d11c3f07  New Richland    MN        Waseca  56072   \n",
       "4  d2f0425877b10ed6bb381f3e2579424a    West Point    VA  King William  23181   \n",
       "\n",
       "        Lat       Lng  ...  TotalCharge Additional_charges Item1 Item2  Item3  \\\n",
       "0  34.34960 -86.72508  ...  3726.702860       17939.403420     3     3      2   \n",
       "1  30.84513 -85.22907  ...  4193.190458       17612.998120     3     4      3   \n",
       "2  43.54321 -96.63772  ...  2434.234222       17505.192460     2     4      4   \n",
       "3  43.89744 -93.51479  ...  2127.830423       12993.437350     3     5      5   \n",
       "4  37.59894 -76.88958  ...  2113.073274        3716.525786     2     1      3   \n",
       "\n",
       "   Item4  Item5 Item6 Item7 Item8  \n",
       "0      2      4     3     3     4  \n",
       "1      4      4     4     3     3  \n",
       "2      4      3     4     3     3  \n",
       "3      3      4     5     5     5  \n",
       "4      3      5     3     4     3  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returning first 5 records of dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returning number of (rows, columns)\n",
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataframe has a total of 10,000 rows and 53 columns. Next, we will return a list of all variables and their dataypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 50 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   CaseOrder           10000 non-null  int64  \n",
      " 1   Customer_id         10000 non-null  object \n",
      " 2   Interaction         10000 non-null  object \n",
      " 3   UID                 10000 non-null  object \n",
      " 4   City                10000 non-null  object \n",
      " 5   State               10000 non-null  object \n",
      " 6   County              10000 non-null  object \n",
      " 7   Zip                 10000 non-null  int64  \n",
      " 8   Lat                 10000 non-null  float64\n",
      " 9   Lng                 10000 non-null  float64\n",
      " 10  Population          10000 non-null  int64  \n",
      " 11  Area                10000 non-null  object \n",
      " 12  TimeZone            10000 non-null  object \n",
      " 13  Job                 10000 non-null  object \n",
      " 14  Children            10000 non-null  int64  \n",
      " 15  Age                 10000 non-null  int64  \n",
      " 16  Income              10000 non-null  float64\n",
      " 17  Marital             10000 non-null  object \n",
      " 18  Gender              10000 non-null  object \n",
      " 19  ReAdmis             10000 non-null  object \n",
      " 20  VitD_levels         10000 non-null  float64\n",
      " 21  Doc_visits          10000 non-null  int64  \n",
      " 22  Full_meals_eaten    10000 non-null  int64  \n",
      " 23  vitD_supp           10000 non-null  int64  \n",
      " 24  Soft_drink          10000 non-null  object \n",
      " 25  Initial_admin       10000 non-null  object \n",
      " 26  HighBlood           10000 non-null  object \n",
      " 27  Stroke              10000 non-null  object \n",
      " 28  Complication_risk   10000 non-null  object \n",
      " 29  Overweight          10000 non-null  object \n",
      " 30  Arthritis           10000 non-null  object \n",
      " 31  Diabetes            10000 non-null  object \n",
      " 32  Hyperlipidemia      10000 non-null  object \n",
      " 33  BackPain            10000 non-null  object \n",
      " 34  Anxiety             10000 non-null  object \n",
      " 35  Allergic_rhinitis   10000 non-null  object \n",
      " 36  Reflux_esophagitis  10000 non-null  object \n",
      " 37  Asthma              10000 non-null  object \n",
      " 38  Services            10000 non-null  object \n",
      " 39  Initial_days        10000 non-null  float64\n",
      " 40  TotalCharge         10000 non-null  float64\n",
      " 41  Additional_charges  10000 non-null  float64\n",
      " 42  Item1               10000 non-null  int64  \n",
      " 43  Item2               10000 non-null  int64  \n",
      " 44  Item3               10000 non-null  int64  \n",
      " 45  Item4               10000 non-null  int64  \n",
      " 46  Item5               10000 non-null  int64  \n",
      " 47  Item6               10000 non-null  int64  \n",
      " 48  Item7               10000 non-null  int64  \n",
      " 49  Item8               10000 non-null  int64  \n",
      "dtypes: float64(7), int64(16), object(27)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Return variables, datatypes and non-null status of each.\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our research question, the pertinent variables are _ReAdmis_, _Doc-visits_, "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Detection and Treatment of Duplicates\n",
    "Our first task is to detect and treat any duplicated values in our entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning a total count of duplicated values\n",
    "df.duplicated().value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see there are **no** duplicated values, represented by the \"False 10000\" output (which is also the total rows shown in the .shape fucntion). We can now move to the next section of data cleaning, detection and treatment of missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Detection and Treatment of Missing Values\n",
    "In this section, we will see if there are any missing values for all variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning a list of variables with total counts for missing values \n",
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there are 7 columns with missing values; **children**, **age**, **income**, **soft_drink**, **overweight**, **anxiety** and **initial_days**\n",
    "<br>\n",
    "<br>1) **Children** and **Age** are considered *discrete quantitative variables* because they can only be particular numbers\n",
    "<br>2) **Income** and **Initial_days** are considered *continuous quantitative variables* because your income can be whole numbers or contain decimals\n",
    "<br>3) **Overweight**, **Soft_drink** and **Anxiety** are considered *nominal qualitative variables* because they are either yes or no\n",
    "\n",
    "We will return some basic statistics on the quantitative variables to check the before and after imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking statistical information on the columns with missing data that are quantative\n",
    "df[['Children', 'Age', 'Income','Initial_days']].describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the seaborn package, we can create histograms of the quantitative variables **Children**, **Age**, **Income**, **Intial_days** to visually\n",
    "analyze their distribution, but before we must import the seaborn package into our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing seaborn package with the inline magic function\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting histograms\n",
    "sns.displot(df, x='Children')\n",
    "sns.displot(df, x='Age')\n",
    "sns.displot(df, x='Income')\n",
    "sns.displot(df, x='Initial_days')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these graphs, we can see that:\n",
    "\n",
    "1) Both **Children** and **Income** are positively skewed to the right\n",
    "2) **Age** is uniformly distributed \n",
    "3) **Initial_days** has a bimodal distribution\n",
    "\n",
    "For **Income**, **Children** and **Initial_days** variables, we will treat missing values by imputation using the median value while for **Age** we will be using the mean for imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing imputation \n",
    "df['Children'].fillna(df['Children'].median(), inplace= True) # Using median value for Children\n",
    "df['Income'].fillna(df['Income'].median(), inplace= True) # Using Median value for Income\n",
    "df['Initial_days'].fillna(df['Initial_days'].median(), inplace= True) # Using median value for Initial_days\n",
    "df['Age'].fillna(df['Age'].mean(), inplace= True) # Performing imputation using mean values form Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking statistics again for comparison\n",
    "df[['Children', 'Age', 'Income','Initial_days']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting histograms again to check for skewness\n",
    "#Plotting histograms\n",
    "sns.displot(df, x='Children')\n",
    "sns.displot(df, x='Age')\n",
    "sns.displot(df, x='Income')\n",
    "sns.displot(df, x='Initial_days')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the data behavior is relatively conserved, evident by both statistical information and histogram.\n",
    "Now we will focus on the remaining categorical variables **Overweight**, **Anxiety**, and **Soft_drink**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting amount of missing values in each\n",
    "print(df['Overweight'].isnull().value_counts())\n",
    "print('') \n",
    "print(df['Soft_drink'].isnull().value_counts())\n",
    "print('')\n",
    "print(df['Anxiety'].isnull().value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with the **Overweight** column - the **Overweight** columns is of the \"0 or 1\" kind. In order to treat missing values, we will calculate the percentage of each category ( 0 and 1) and impute missing values with the highest percentage category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining value counts for each category in the Overweight column\n",
    "df['Overweight'].value_counts(normalize=True, sort=True) # We are using the parameter normalize to return the proportions rather than frequencies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a nearly 70/30 split for 1:0 ratio - this means we can impute the missing values using '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating missing values in Overweight column with '1' (highest percentage category)\n",
    "df['Overweight'].fillna(1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will follow the same procedure for the **Anxiety** column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining value counts for each category in the Overweight column\n",
    "df['Anxiety'].value_counts(normalize=True, sort=True) # We are using the parameter normalize to return the proportions rather than frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating missing values in Anxiety column with '0' (highest percentage category)\n",
    "df['Anxiety'].fillna(0, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now focus on the **Soft-Drink** column which is of the 'Yes/No' type - we will first re-express the variable to '0/1' using Ordinal Encoding and then using the methods used above for **Overweight** and **Anxiety** to fill is missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will run the .unique() function to determine number of unique values for specified variables\n",
    "df['Soft_drink'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the unique values are 'No', 'Yes' and _nan_ for missing values. We will use the percentage method mentioned above to fill these in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining value counts for each category in the Overweight column\n",
    "df['Soft_drink'].value_counts(normalize=True, sort=True, dropna=True) # not including the NaN values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a 75/25 split for a 1:0 ratio - we will use 'No' to fill in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating missing values in Soft_drink column with 'No' (highest percentage category)\n",
    "df['Soft_drink'].fillna('No', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechecking for any missing values\n",
    "df['Soft_drink'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now begin the ordinal encoding process..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate the variable in preparation for replacing its categorical values with\n",
    "# numeric ones. This replicated variable will store the re-expressed values once converted\n",
    "\n",
    "df['Soft_drink_numeric'] = df['Soft_drink']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking duplicated column 'Soft_drink_numeric' vs original \"Soft_drink\"\n",
    "df[['Soft_drink_numeric', 'Soft_drink']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a dictionary specifically for converting the categorical values to numeric values.\n",
    "dict_soft_drink = {'Soft_drink_numeric': {'No': 0, 'Yes': 1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the dictionary to replace the variable’s values. The replacefunction will replace the values according to the rules in\n",
    "# the dictionary dict_edu and store in existing data frame.\n",
    "df.replace(dict_soft_drink, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have detected and treated all missing values. We will now see the dataframe overall and run .info() to see if any null-values exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking info on dataframe\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, there are no more variables with missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Detection and Treatment of Outliers\n",
    "In this section, we will check for any outliers in the quantitative variables and treat them accordingly.\n",
    "Let's first visualize the boxplots for the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Children boxplot\n",
    "sns.boxplot(x='Children', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age boxplot\n",
    "sns.boxplot(x='Age', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income boxplot\n",
    "sns.boxplot(x='Income', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial_days boxplot\n",
    "sns.boxplot(x='Initial_days', data=df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that **Age** and **Initial_days** have no outliers while **Children** has 4 outliers and **Income** has multiple. Since we do not know if the outliers are factual errors, we will first extract the outliers, save them as their own dataframe and then remove them from the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantity of outliers in the Income column\n",
    "df['Income'][df['Income'] > 46466.7975].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting basic statistical info on Income and Children columns\n",
    "df['Income'].describe()\n",
    "df['Children'].describe()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the z-scores to extract all records whose z-score is greater than 3. We must first import the SciPy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improting Scipy package\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column for the Income z-scores and Children z-scores\n",
    "df['Income_z_Scores'] = stats.zscore(df['Income'])\n",
    "df['Children_z_Scores'] = stats.zscore(df['Children'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing first 10 records for both Income and Children z-score columns\n",
    "df[['Income', 'Income_z_Scores']].head(10)\n",
    "df[['Children', 'Children_z_Scores']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting records with z-scores -3 < z and z > 3 and saving as new variable \n",
    "income_outliers = df.query('Income_z_Scores < -3 | Income_z_Scores > 3')\n",
    "children_outliers = df.query('Children_z_Scores < -3 | Children_z_Scores > 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with Children and Income outliers removed and saving as df_new\n",
    "df_new = df[(df['Income_z_Scores'] > -3) & (df['Income_z_Scores'] < 3) & (df['Children_z_Scores'] > -3) & (df['Children_z_Scores'] < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking z-scores in df_new for any z-scores missed in both Income and Children column\n",
    "df_new['Children_z_Scores'].loc[lambda x : (x < -3) | (x > 3)].sum()\n",
    "df_new['Income_z_Scores'].loc[lambda x : (x < -3) | (x > 3)].sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction to CSV\n",
    "\n",
    "We will now extract our treated dataframe into a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting dataframe 'df_new' into a CSV file named medical_data_treated\n",
    "medical_data_treated = df_new.to_csv('Medical_Data_Treated.csv', index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Principal Component Analysis (PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA # to apply PCA\n",
    "from sklearn.preprocessing import StandardScaler # to standardize features\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though it is not required, we will visualize the variable correlations using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize = (10,10)) # Creating slightly larger heatmap for ease of visualization\n",
    "mask = np.triu(np.ones_like(df_new.corr(), dtype=bool)) # Creating heatmap mask\n",
    "sns.heatmap(df_new.corr(), square=True, xticklabels=True, yticklabels=True, cmap='vlag', mask=mask) # Heatmap parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleting only numeric columns and saving to new dataframe \n",
    "df_new_numeric = df_new.select_dtypes(include=np.number) \n",
    "df_new_numeric.dtypes # Checking to see if all variables are numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the variables to perform PCA and naming variable test_pca\n",
    "test_pca = df_new_numeric[['Population','Children', 'Age', 'Income', 'Doc_visits', 'Initial_days', 'TotalCharge', 'Additional_charges', 'VitD_levels', 'Doc_visits', 'Full_meals_eaten']]\n",
    "test_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing test_pca dataframe\n",
    "test_pca_normalized = (test_pca-test_pca.mean()) / test_pca.std()\n",
    "test_pca_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating pca object\n",
    "pca = PCA(n_components=test_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting data\n",
    "pca.fit(test_pca_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new dataframe with PCs\n",
    "test_pca2 = pd.DataFrame(pca.transform(test_pca_normalized), columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11'])\n",
    "test_pca2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating loadings\n",
    "loadings = pd.DataFrame(pca.components_.T,\n",
    "                        columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8', 'PC9', 'PC10', 'PC11'],\n",
    "                        index = test_pca_normalized.columns)\n",
    "loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating covariance matrix and eigenvalues\n",
    "cov_matrix = np.dot(test_pca_normalized.T, test_pca_normalized) / test_pca.shape[0]\n",
    "eigenvalues = [np.dot(eigenvector.T, np.dot(cov_matrix,eigenvector)) for eigenvector in pca.components_]\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting eigenvalues\n",
    "plt.plot(eigenvalues)\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('EigenValues')\n",
    "plt.axhline(y=1, color='red') # Visualizing eigenvalue = 1 to use in Kaiser Rule\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23ecf8896591abdb4f319a996273d77760c5f800ba007ba21e578a28215e94bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
